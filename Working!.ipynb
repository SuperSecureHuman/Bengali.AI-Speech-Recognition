{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# disable GPU\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from datasets import Dataset as HFDataset, Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_df(root_path):\n",
    "    # Read the TSV file and extract data\n",
    "    tsv_file_path = os.path.join(root_path, \"utt_spk_text.tsv\")\n",
    "    with open(tsv_file_path, \"r\", encoding=\"utf-8\") as tsv_file:\n",
    "        lines = tsv_file.readlines()\n",
    "\n",
    "    # Prepare data for the Hugging Face dataset\n",
    "    file_paths = []\n",
    "    folder_names = []\n",
    "    texts = []\n",
    "\n",
    "    for line in lines:\n",
    "        file_name, _, text = line.strip().split(\"\\t\")\n",
    "        folder_name = file_name[:2]\n",
    "\n",
    "        file_path = os.path.join(root_path, \"data\", folder_name, file_name + \".flac\")\n",
    "\n",
    "        file_paths.append(file_path)\n",
    "        folder_names.append(folder_name)\n",
    "        texts.append(text)\n",
    "\n",
    "    # Create the Hugging Face dataset\n",
    "    dataset_dict = {\n",
    "        \"file_path\": file_paths,\n",
    "        \"folder_name\": folder_names,\n",
    "        \"text\": texts,\n",
    "    }\n",
    "    \n",
    "    # create dataframe\n",
    "    df = pd.DataFrame(dataset_dict)\n",
    "    \n",
    "    return df, dataset_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df,dataset_dict = prep_df(\"/home/venom/repo/Bengali.AI-Speech-Recognition/openslr/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           file_path folder_name  \\\n",
      "0  /home/venom/repo/Bengali.AI-Speech-Recognition...          00   \n",
      "1  /home/venom/repo/Bengali.AI-Speech-Recognition...          00   \n",
      "2  /home/venom/repo/Bengali.AI-Speech-Recognition...          00   \n",
      "3  /home/venom/repo/Bengali.AI-Speech-Recognition...          00   \n",
      "4  /home/venom/repo/Bengali.AI-Speech-Recognition...          00   \n",
      "\n",
      "                      text  \n",
      "0  বাংলাদেশে দায়িত্ব নেবে  \n",
      "1      এ ধরণের কার্ড নিয়ে  \n",
      "2        হতে উপার্জিত অর্থ  \n",
      "3    হাসির বিষয় হয়েই আছে  \n",
      "4          সার্ক দেশগুলোতে  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor\n",
    "\n",
    "\n",
    "# Load the tokenizer and feature extractor\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"bengali\", task=\"transcribe\")\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"bengali\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the dictionary in a Hugging Face Dataset object\n",
    "dataset_output = HFDataset.from_dict(dataset_dict)\n",
    "\n",
    "# Cast the 'file_path' column to the 'Audio' feature\n",
    "dataset = dataset_output.cast_column(\"file_path\", Audio())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test train split on dataset\n",
    "# dataset = load_from_disk(\"./dataset\")\n",
    "dataset = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import librosa\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from datasets import IterableDataset\n",
    "\n",
    "# fraction = 0.00001\n",
    "\n",
    "# num_samples = len(df)\n",
    "# num_test_samples = int(fraction * num_samples)\n",
    "# num_train_samples = num_samples - num_test_samples\n",
    "\n",
    "# # Split the dataset into train, test, and validation sets.\n",
    "# train_set, test_set = train_test_split(df, train_size=num_train_samples, random_state=42)\n",
    "\n",
    "# sampling_rate = 16000\n",
    "# #path_template = \"/home/venom/repo/Bengali.AI-Speech-Recognition/{}\"\n",
    "# path_template = \"{}\"\n",
    "\n",
    "\n",
    "\n",
    "# def dataset_generator(df):\n",
    "#     for _, row in df.iterrows():\n",
    "#         audio_array = librosa.load(path_template.format(row[\"file_path\"]))[0]\n",
    "#         yield {\n",
    "#             \"input_features\": feature_extractor(audio_array, sampling_rate=sampling_rate).input_features[0], \n",
    "#             \"labels\": tokenizer(row[\"text\"]).input_ids\n",
    "#         }\n",
    "        \n",
    "# train_ds = IterableDataset.from_generator(dataset_generator, gen_kwargs={\"df\": train_set})\n",
    "# eval_ds = IterableDataset.from_generator(dataset_generator, gen_kwargs={\"df\": test_set})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['file_path', 'folder_name', 'text'],\n",
       "        num_rows: 218700\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['file_path', 'folder_name', 'text'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    feature_extractor: Any\n",
    "    tokenizer: Any\n",
    "    sampling_rate: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # First treat the audio inputs: load audio, extract features and convert to torch tensors\n",
    "        input_features = [{\"input_features\": self.feature_extractor(feature[\"file_path\"][\"array\"], sampling_rate=self.sampling_rate).input_features[0]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": self.tokenizer(feature[\"text\"]).input_ids} for feature in features]\n",
    "        # Pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # If bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's appended later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor, feature_extractor=feature_extractor, tokenizer=tokenizer, sampling_rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-29 09:13:43.564825: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-29 09:13:43.601411: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    #metric.remove_unk_tokens = True\n",
    "    \n",
    "    #pred_ids = pred_ids[0]\n",
    "    \n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"bangla-speech-processing/BanglaASR\") #,load_in_8bit=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from peft import prepare_model_for_int8_training\n",
    "\n",
    "# model = prepare_model_for_int8_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import bitsandbytes as bnb\n",
    "\n",
    "# def find_all_linear_names(model, bits=8):\n",
    "#     cls = bnb.nn.Linear4bit if bits == 4 else (bnb.nn.Linear8bitLt if bits == 8 else torch.nn.Linear)\n",
    "#     lora_module_names = set()\n",
    "#     for name, module in model.named_modules():\n",
    "#         if isinstance(module, cls):\n",
    "#             names = name.split('.')\n",
    "#             lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "\n",
    "#     if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "#         lora_module_names.remove('lm_head')\n",
    "#     return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_modules = find_all_linear_names(model)\n",
    "target_modules = ['k_proj', 'fc2', 'q_proj', 'fc1', 'out_proj', 'v_proj']\n",
    "target_modules = ['q_proj', 'v_proj']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig,LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(r=8, lora_alpha=32, target_modules=target_modules, lora_dropout=0.1, bias=\"none\")\n",
    "\n",
    "\n",
    "if hasattr(model, \"enable_input_require_grads\"):\n",
    "    model.enable_input_require_grads()\n",
    "else:\n",
    "    def make_inputs_require_grad(module, input, output):\n",
    "         output.requires_grad_(True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 884,736 || all params: 242,619,648 || trainable%: 0.36465966680489126\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-small-bn\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=30,\n",
    "    #max_steps=10,\n",
    "    do_eval=True,\n",
    "    do_train=True,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    #use_ipex=True,\n",
    "    #bf16=False,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=1,\n",
    "    generation_max_length=225,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    logging_steps=25,\n",
    "    report_to=\"tensorboard\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    save_total_limit=5,\n",
    "    remove_unused_columns=False,  # required as the PeftModel forward doesn't have the signature of the wrapped model's forward\n",
    "    label_names=[\"labels\"],  # same reason as above\n",
    "    predict_with_generate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    \n",
    ")\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dbb227556d84119931c439c1eab7ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "549ee1582c4c4e9a9b047ab772b1bed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5379034876823425, 'eval_wer': 62.5, 'eval_runtime': 1.7695, 'eval_samples_per_second': 1.695, 'eval_steps_per_second': 1.695, 'epoch': 0.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e78aa050d7634312860655e32b0d3252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5379326939582825, 'eval_wer': 62.5, 'eval_runtime': 1.667, 'eval_samples_per_second': 1.8, 'eval_steps_per_second': 1.8, 'epoch': 0.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "593adc591bd8408c908eb3674e7aaa4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
